{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ScienceQA vPGM evaluation (129 questions)\n",
        "Evaluate vPGM output quality on 129 ScienceQA questions, report accuracy/calibration, and prepare comparisons against alternative prompting strategies.\n",
        "\n",
        "> To run locally, activate your environment first: `source C:/Users/shafi/anaconda3/Scripts/activate PGM`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shafi\\anaconda3\\envs\\PGM\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List, Optional, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load vPGM results and ScienceQA ground truth\n",
        "The notebook expects a JSONL file named `scienceqa_vpgm_results_129.jsonl` in the working directory (one object per line). If that file is missing, it will fall back to `scienceqa_vpgm_results.jsonl`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Primary file scienceqa_vpgm_results_129.jsonl not found; falling back to scienceqa_vpgm_results.jsonl.\n",
            "Loaded 129 vPGM instances from scienceqa_vpgm_results.jsonl\n"
          ]
        }
      ],
      "source": [
        "results_path = Path(\"scienceqa_vpgm_results_129.jsonl\")\n",
        "if not results_path.exists():\n",
        "    alt_path = Path(\"scienceqa_vpgm_results.jsonl\")\n",
        "    if alt_path.exists():\n",
        "        print(f\"Primary file {results_path} not found; falling back to {alt_path}.\")\n",
        "        results_path = alt_path\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Could not find {results_path}. Place the vPGM JSONL file in the working directory.\")\n",
        "\n",
        "instances: List[Dict[str, Any]] = []\n",
        "with results_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        instances.append(json.loads(line))\n",
        "\n",
        "print(f\"Loaded {len(instances)} vPGM instances from {results_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available splits: {'train': 12726, 'validation': 4241, 'test': 4241}\n",
            "Mapped 0 unique ScienceQA questions across splits\n"
          ]
        }
      ],
      "source": [
        "dataset_name = \"derek-thomas/ScienceQA\"\n",
        "raw_dataset = load_dataset(dataset_name)\n",
        "\n",
        "def extract_question_id(example: Dict[str, Any]) -> Optional[str]:\n",
        "    for key in (\"scienceqa_id\", \"id\", \"qid\", \"question_id\", \"questionid\", \"question_id_str\"):\n",
        "        if key in example and example[key] is not None:\n",
        "            return str(example[key])\n",
        "    return None\n",
        "\n",
        "id_to_example: Dict[str, Dict[str, Any]] = {}\n",
        "for split_name, split_ds in raw_dataset.items():\n",
        "    for ex in split_ds:\n",
        "        qid = extract_question_id(ex)\n",
        "        if qid is None:\n",
        "            continue\n",
        "        id_to_example[qid] = ex\n",
        "\n",
        "split_sizes = {name: len(ds) for name, ds in raw_dataset.items()}\n",
        "print(f\"Available splits: {split_sizes}\")\n",
        "print(f\"Mapped {len(id_to_example)} unique ScienceQA questions across splits\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build evaluation table\n",
        "Helpers below align vPGM outputs with ScienceQA choices, resolve predicted answers, and assemble a tidy DataFrame for analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_text(value: Any) -> str:\n",
        "    return str(value).strip().lower()\n",
        "\n",
        "def option_label_to_index(label: Any, num_options: int) -> Optional[int]:\n",
        "    if label is None:\n",
        "        return None\n",
        "    text = str(label).strip()\n",
        "    if text.isdigit():\n",
        "        idx = int(text)\n",
        "        if 0 <= idx < num_options:\n",
        "            return idx\n",
        "        if 1 <= idx <= num_options:\n",
        "            return idx - 1  # tolerate 1-based labels\n",
        "    if len(text) == 1 and text.isalpha():\n",
        "        idx = ord(text.lower()) - ord(\"a\")\n",
        "        if 0 <= idx < num_options:\n",
        "            return idx\n",
        "    return None\n",
        "\n",
        "def find_choice_index_by_text(option_text: Any, choices: List[Any]) -> int:\n",
        "    target = normalize_text(option_text)\n",
        "    for idx, choice in enumerate(choices):\n",
        "        if normalize_text(choice) == target:\n",
        "            return idx\n",
        "    return -1\n",
        "\n",
        "def resolve_prob_vector(option_probs: Dict[str, float], observed_options: List[Any], choices: List[Any]) -> np.ndarray:\n",
        "    prob_vector = np.zeros(len(choices), dtype=float)\n",
        "    for key, prob in option_probs.items():\n",
        "        idx_from_label = option_label_to_index(key, len(observed_options))\n",
        "        mapped_idx = None\n",
        "        if idx_from_label is not None and idx_from_label < len(observed_options):\n",
        "            mapped_idx = find_choice_index_by_text(observed_options[idx_from_label], choices)\n",
        "        if mapped_idx is None or mapped_idx < 0:\n",
        "            mapped_idx = find_choice_index_by_text(key, choices)\n",
        "        if mapped_idx is None or mapped_idx < 0 or mapped_idx >= len(choices):\n",
        "            continue\n",
        "        prob_vector[mapped_idx] = float(prob)\n",
        "    if prob_vector.sum() > 0:\n",
        "        prob_vector = prob_vector / prob_vector.sum()\n",
        "    return prob_vector\n",
        "\n",
        "def resolve_selected_answer(selected_answer: Any, observed_options: List[Any], choices: List[Any], prob_vector: Optional[np.ndarray] = None) -> Tuple[Any, Optional[int]]:\n",
        "    if selected_answer is None:\n",
        "        selected_answer = \"\"\n",
        "    direct_idx = find_choice_index_by_text(selected_answer, choices)\n",
        "    if direct_idx >= 0:\n",
        "        return choices[direct_idx], direct_idx\n",
        "    idx_from_label = option_label_to_index(selected_answer, len(choices))\n",
        "    if idx_from_label is not None:\n",
        "        return choices[idx_from_label], idx_from_label\n",
        "    obs_idx = find_choice_index_by_text(selected_answer, observed_options)\n",
        "    if obs_idx >= 0:\n",
        "        matched_idx = find_choice_index_by_text(observed_options[obs_idx], choices)\n",
        "        if matched_idx >= 0:\n",
        "            return choices[matched_idx], matched_idx\n",
        "    if prob_vector is not None and prob_vector.size:\n",
        "        idx = int(np.argmax(prob_vector))\n",
        "        return choices[idx], idx\n",
        "    return selected_answer, None\n",
        "\n",
        "def compute_brier_score_row(prob_vector: np.ndarray, correct_idx: Optional[int]) -> float:\n",
        "    if correct_idx is None or prob_vector.size == 0:\n",
        "        return np.nan\n",
        "    y = np.zeros_like(prob_vector)\n",
        "    if 0 <= correct_idx < prob_vector.size:\n",
        "        y[correct_idx] = 1.0\n",
        "    return float(np.mean((prob_vector - y) ** 2))\n",
        "\n",
        "def build_reliability_curve(df: pd.DataFrame, n_bins: int = 10) -> pd.DataFrame:\n",
        "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "    tmp = df.copy()\n",
        "    tmp = tmp.dropna(subset=[\"pred_confidence\"])\n",
        "    tmp[\"conf_bin\"] = pd.cut(tmp[\"pred_confidence\"], bins=bins, include_lowest=True, labels=False)\n",
        "    grouped = tmp.groupby(\"conf_bin\")\n",
        "    curve = grouped.apply(\n",
        "        lambda g: pd.Series(\n",
        "            {\n",
        "                \"bin_count\": len(g),\n",
        "                \"bin_confidence_mean\": g[\"pred_confidence\"].mean(),\n",
        "                \"bin_accuracy\": g[\"is_correct\"].mean(),\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "    curve[\"bin_left\"] = bins[:-1]\n",
        "    curve[\"bin_right\"] = bins[1:]\n",
        "    return curve.reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Constructed DataFrame with 0 rows; 0 have ground truth available for evaluation.\n",
            "Warning: 129 instances missing ground truth mapping (first few: ['', '', '', '', ''])\n",
            "No rows were constructed. Check that scienceqa_id values in the results file match the ScienceQA dataset ids.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>scienceqa_id</th>\n",
              "      <th>subject</th>\n",
              "      <th>topic</th>\n",
              "      <th>category</th>\n",
              "      <th>skill</th>\n",
              "      <th>grade</th>\n",
              "      <th>pred_selected_answer</th>\n",
              "      <th>pred_option_probs</th>\n",
              "      <th>pred_confidence</th>\n",
              "      <th>gt_answer_index</th>\n",
              "      <th>gt_answer_text</th>\n",
              "      <th>is_correct</th>\n",
              "      <th>brier_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [scienceqa_id, subject, topic, category, skill, grade, pred_selected_answer, pred_option_probs, pred_confidence, gt_answer_index, gt_answer_text, is_correct, brier_score]\n",
              "Index: []"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rows: List[Dict[str, Any]] = []\n",
        "missing_ground_truth: List[str] = []\n",
        "\n",
        "for inst in instances:\n",
        "    meta = inst.get(\"question_meta\", {})\n",
        "    scienceqa_id_raw = meta.get(\"scienceqa_id\")\n",
        "    scienceqa_id = str(scienceqa_id_raw) if scienceqa_id_raw is not None else None\n",
        "    example = id_to_example.get(scienceqa_id)\n",
        "    if example is None:\n",
        "        missing_ground_truth.append(scienceqa_id)\n",
        "        continue\n",
        "\n",
        "    choices = example.get(\"choices\", [])\n",
        "    gt_answer_index = example.get(\"answer\")\n",
        "    gt_answer_text = None\n",
        "    if gt_answer_index is not None and 0 <= gt_answer_index < len(choices):\n",
        "        gt_answer_text = choices[gt_answer_index]\n",
        "\n",
        "    observed_options = inst.get(\"observed\", {}).get(\"options\", [])\n",
        "    option_probs = inst.get(\"answer_posterior\", {}).get(\"option_probabilities\", {})\n",
        "    prob_vector = resolve_prob_vector(option_probs, observed_options, choices)\n",
        "    pred_confidence = float(np.nanmax(prob_vector)) if prob_vector.size else np.nan\n",
        "\n",
        "    selected_answer = inst.get(\"answer_posterior\", {}).get(\"selected_answer\")\n",
        "    pred_answer_text, pred_answer_index = resolve_selected_answer(\n",
        "        selected_answer, observed_options, choices, prob_vector\n",
        "    )\n",
        "\n",
        "    is_correct = np.nan\n",
        "    if pred_answer_index is not None and gt_answer_index is not None:\n",
        "        is_correct = pred_answer_index == gt_answer_index\n",
        "\n",
        "    brier_score = compute_brier_score_row(prob_vector, gt_answer_index)\n",
        "\n",
        "    rows.append(\n",
        "        {\n",
        "            \"scienceqa_id\": scienceqa_id,\n",
        "            \"subject\": meta.get(\"subject\"),\n",
        "            \"topic\": meta.get(\"topic\"),\n",
        "            \"category\": meta.get(\"category\"),\n",
        "            \"skill\": meta.get(\"skill\"),\n",
        "            \"grade\": meta.get(\"grade\"),\n",
        "            \"pred_selected_answer\": pred_answer_text,\n",
        "            \"pred_option_probs\": {str(choice): float(prob_vector[i]) for i, choice in enumerate(choices)},\n",
        "            \"pred_confidence\": pred_confidence,\n",
        "            \"gt_answer_index\": gt_answer_index,\n",
        "            \"gt_answer_text\": gt_answer_text,\n",
        "            \"is_correct\": is_correct,\n",
        "            \"brier_score\": brier_score,\n",
        "        }\n",
        "    )\n",
        "\n",
        "columns = [\n",
        "    \"scienceqa_id\",\n",
        "    \"subject\",\n",
        "    \"topic\",\n",
        "    \"category\",\n",
        "    \"skill\",\n",
        "    \"grade\",\n",
        "    \"pred_selected_answer\",\n",
        "    \"pred_option_probs\",\n",
        "    \"pred_confidence\",\n",
        "    \"gt_answer_index\",\n",
        "    \"gt_answer_text\",\n",
        "    \"is_correct\",\n",
        "    \"brier_score\",\n",
        "]\n",
        "df = pd.DataFrame(rows, columns=columns)\n",
        "\n",
        "eval_df = df.dropna(subset=[\"gt_answer_text\"]).copy() if not df.empty else df.copy()\n",
        "\n",
        "print(f\"Constructed DataFrame with {len(df)} rows; {len(eval_df)} have ground truth available for evaluation.\")\n",
        "if missing_ground_truth:\n",
        "    print(f\"Warning: {len(missing_ground_truth)} instances missing ground truth mapping (first few: {missing_ground_truth[:5]})\")\n",
        "if df.empty:\n",
        "    print(\"No rows were constructed. Check that scienceqa_id values in the results file match the ScienceQA dataset ids.\")\n",
        "\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics: accuracy, Brier score, and calibration\n",
        "Compute overall accuracy, per-subject accuracy, Brier score, and reliability curve (calibration).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No evaluable rows found. Verify that the results file has ScienceQA ids matching the dataset and rerun.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bin_count</th>\n",
              "      <th>bin_confidence_mean</th>\n",
              "      <th>bin_accuracy</th>\n",
              "      <th>bin_left</th>\n",
              "      <th>bin_right</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [bin_count, bin_confidence_mean, bin_accuracy, bin_left, bin_right]\n",
              "Index: []"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "overall_accuracy = float(\"nan\")\n",
        "subject_accuracy = pd.Series(dtype=float)\n",
        "mean_brier_score = float(\"nan\")\n",
        "calibration_curve = pd.DataFrame(columns=[\"bin_count\", \"bin_confidence_mean\", \"bin_accuracy\", \"bin_left\", \"bin_right\"])\n",
        "avg_calibration_gap = float(\"nan\")\n",
        "\n",
        "if eval_df.empty:\n",
        "    print(\"No evaluable rows found. Verify that the results file has ScienceQA ids matching the dataset and rerun.\")\n",
        "else:\n",
        "    overall_accuracy = eval_df[\"is_correct\"].mean()\n",
        "    subject_accuracy = eval_df.groupby(\"subject\")[\"is_correct\"].mean().sort_values(ascending=False)\n",
        "    mean_brier_score = eval_df[\"brier_score\"].mean()\n",
        "    calibration_curve = build_reliability_curve(eval_df, n_bins=10)\n",
        "    calibration_curve[\"confidence_minus_accuracy\"] = (\n",
        "        calibration_curve[\"bin_confidence_mean\"] - calibration_curve[\"bin_accuracy\"]\n",
        "    )\n",
        "    avg_calibration_gap = calibration_curve[\"confidence_minus_accuracy\"].mean()\n",
        "\n",
        "    print(f\"Overall accuracy: {overall_accuracy:.3f}\")\n",
        "    print(f\"Mean Brier score: {mean_brier_score:.3f}\")\n",
        "    print(f\"Average calibration gap (confidence - accuracy): {avg_calibration_gap:.3f}\")\n",
        "    print(\"Accuracy by subject:\")\n",
        "    display(subject_accuracy)\n",
        "\n",
        "calibration_curve\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizations\n",
        "The plots below show model confidence distribution, calibration (reliability diagram), accuracy by subject, and Brier score distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No confidence data to plot.\n"
          ]
        }
      ],
      "source": [
        "if eval_df.empty:\n",
        "    print(\"No confidence data to plot.\")\n",
        "else:\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    sns.histplot(eval_df[\"pred_confidence\"].dropna(), bins=10, color=\"steelblue\", edgecolor=\"white\")\n",
        "    plt.xlabel(\"Predicted confidence (max probability)\")\n",
        "    plt.ylabel(\"Count of questions\")\n",
        "    plt.title(\"vPGM confidence distribution (129 questions)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if calibration_curve.empty:\n",
        "    print(\"No calibration data to plot.\")\n",
        "else:\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.plot(\n",
        "        calibration_curve[\"bin_confidence_mean\"],\n",
        "        calibration_curve[\"bin_accuracy\"],\n",
        "        marker=\"o\",\n",
        "        linestyle=\"-\",\n",
        "        label=\"vPGM\"\n",
        "    )\n",
        "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Perfect calibration\")\n",
        "    plt.xlabel(\"Mean predicted confidence\")\n",
        "    plt.ylabel(\"Empirical accuracy\")\n",
        "    plt.title(\"Reliability diagram for vPGM answers (129 questions)\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if subject_accuracy.empty:\n",
        "    print(\"No subject-level accuracy data to plot.\")\n",
        "else:\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    subj_order = subject_accuracy.index\n",
        "    sns.barplot(x=subj_order, y=subject_accuracy.values, palette=\"viridis\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.xlabel(\"Subject\")\n",
        "    plt.title(\"Accuracy by subject\")\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if eval_df.empty:\n",
        "    print(\"No Brier score data to plot.\")\n",
        "else:\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    sns.histplot(eval_df[\"brier_score\"].dropna(), bins=10, color=\"darkorange\", edgecolor=\"white\")\n",
        "    plt.xlabel(\"Per-question Brier score\")\n",
        "    plt.ylabel(\"Count of questions\")\n",
        "    plt.title(\"Distribution of Brier scores\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and comparisons\n",
        "The cell below auto-generates a brief summary once the notebook has been run. Use it to interpret accuracy, calibration, and subject-wise differences. Add notes comparing against other prompting pipelines (Plain GPT-4 CoT, GPT-4 CoT + vPGM, structure-free prompting baselines, and standard VQA baselines) after you run those baselines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "    top_subjects = subject_accuracy.head(3)\n",
        "    summary_lines = [\n",
        "        f\"- Overall accuracy on evaluated set: **{overall_accuracy:.3f}**\",\n",
        "        f\"- Mean Brier score: **{mean_brier_score:.3f}**\",\n",
        "        f\"- Average calibration gap (confidence - accuracy): **{avg_calibration_gap:.3f}** (positive = overconfident)\",\n",
        "        f\"- Top subjects by accuracy: {', '.join([f'{subj} ({acc:.2f})' for subj, acc in top_subjects.items()])}\",\n",
        "        \"- Check the reliability diagram for signs of over/underconfidence; large positive gaps indicate overconfidence.\",\n",
        "        \"- Add baseline results here (Plain GPT-4 CoT, GPT-4 CoT + vPGM, structure-free prompting, VQA baselines) to contextualize vPGM performance.\"\n",
        "    ]\n",
        "\n",
        "    display(Markdown(\"\n",
        "\".join([\"### Quick takeaways\"] + summary_lines)))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PGM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
