{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# P-GALM Framework Evaluation\n",
                "\n",
                "This notebook evaluates the Probabilistic Graph-Augmented Language Model (P-GALM) on a subset of the ScienceQA dataset. \n",
                "\n",
                "It performs the following steps:\n",
                "1.  Loads the ScienceQA validation dataset.\n",
                "2.  Selects a random sample of questions.\n",
                "3.  Runs the vPGM inference pipeline on each question.\n",
                "4.  Compares the predicted answer with the ground truth.\n",
                "5.  Calculates accuracy and displays detailed results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\shafi\\anaconda3\\envs\\PGM\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import json\n",
                "import random\n",
                "import pandas as pd\n",
                "from tqdm import tqdm\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# Load environment variables (OPENAI_API_KEY)\n",
                "load_dotenv()\n",
                "\n",
                "# Import P-GALM modules\n",
                "from scienceqa_vpgm_loader import load_scienceqa, load_prompt_template, build_scienceqa_skeleton\n",
                "from vpgm_llm_client import infer_vpgm_for_skeleton"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "SAMPLE_SIZE = 10  # Number of questions to evaluate\n",
                "SPLIT = \"validation\"\n",
                "TEMPLATE_ID = \"scienceqa_vpgm_4latent_generic\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading dataset and template...\n",
                        "Loaded 4241 examples from validation split.\n"
                    ]
                }
            ],
            "source": [
                "print(\"Loading dataset and template...\")\n",
                "dataset = load_scienceqa(split=SPLIT)\n",
                "template = load_prompt_template()\n",
                "print(f\"Loaded {len(dataset)} examples from {SPLIT} split.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found 2144 text-only questions.\n",
                        "Selected 10 questions for evaluation.\n"
                    ]
                }
            ],
            "source": [
                "# Select a random sample\n",
                "# Filter out images for this evaluation to focus on text reasoning, or keep them if you want full eval\n",
                "# For this demo, we'll try to pick text-only questions to avoid potential image processing issues if any,\n",
                "# but the pipeline supports images if they are handled correctly.\n",
                "\n",
                "candidates = [ex for ex in dataset if ex.get('image') is None]\n",
                "print(f\"Found {len(candidates)} text-only questions.\")\n",
                "\n",
                "if len(candidates) < SAMPLE_SIZE:\n",
                "    sample = candidates\n",
                "else:\n",
                "    sample = random.sample(candidates, SAMPLE_SIZE)\n",
                "\n",
                "print(f\"Selected {len(sample)} questions for evaluation.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting inference...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 10/10 [02:05<00:00, 12.57s/it]\n"
                    ]
                }
            ],
            "source": [
                "results = []\n",
                "\n",
                "print(\"Starting inference...\")\n",
                "for i, ex in enumerate(tqdm(sample)):\n",
                "    # 1. Build Skeleton\n",
                "    # Use index as synthetic ID if needed\n",
                "    sqa_id = ex.get(\"id\") or ex.get(\"qid\") or f\"sample_{i}\"\n",
                "    skeleton = build_scienceqa_skeleton(ex, TEMPLATE_ID, template, override_id=sqa_id)\n",
                "    \n",
                "    # 2. Run Inference\n",
                "    try:\n",
                "        prediction_result = infer_vpgm_for_skeleton(skeleton, template, template_id=TEMPLATE_ID)\n",
                "        \n",
                "        # 3. Extract Answer\n",
                "        # The model returns 'selected_answer' which is the text of the option.\n",
                "        # We need to map it back to the index (0, 1, 2...)\n",
                "        selected_text = prediction_result[\"answer_posterior\"][\"selected_answer\"]\n",
                "        options = ex[\"choices\"]\n",
                "        \n",
                "        # Simple exact match or substring match\n",
                "        pred_index = -1\n",
                "        if selected_text in options:\n",
                "            pred_index = options.index(selected_text)\n",
                "        else:\n",
                "            # Fallback: try to find which option is contained in the selected text\n",
                "            for idx, opt in enumerate(options):\n",
                "                if opt in selected_text or selected_text in opt:\n",
                "                    pred_index = idx\n",
                "                    break\n",
                "        \n",
                "        # 4. Compare with Ground Truth\n",
                "        ground_truth_index = ex[\"answer\"]\n",
                "        is_correct = (pred_index == ground_truth_index)\n",
                "        \n",
                "        results.append({\n",
                "            \"id\": sqa_id,\n",
                "            \"question\": ex[\"question\"],\n",
                "            \"ground_truth_index\": ground_truth_index,\n",
                "            \"ground_truth_text\": options[ground_truth_index],\n",
                "            \"predicted_index\": pred_index,\n",
                "            \"predicted_text\": selected_text,\n",
                "            \"is_correct\": is_correct,\n",
                "            \"reasoning_quality\": prediction_result[\"latent_posteriors\"][\"Z4_logical_reasoning\"][\"justification\"]\n",
                "        })\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Error processing {sqa_id}: {e}\")\n",
                "        results.append({\n",
                "            \"id\": sqa_id,\n",
                "            \"question\": ex[\"question\"],\n",
                "            \"ground_truth_index\": ex[\"answer\"],\n",
                "            \"ground_truth_text\": ex[\"choices\"][ex[\"answer\"]],\n",
                "            \"predicted_index\": -1,\n",
                "            \"predicted_text\": \"ERROR\",\n",
                "            \"is_correct\": False,\n",
                "            \"reasoning_quality\": str(e)\n",
                "        })"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Overall Accuracy: 100.00%\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>question</th>\n",
                            "      <th>ground_truth_text</th>\n",
                            "      <th>predicted_text</th>\n",
                            "      <th>is_correct</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>Suppose Josiah decides to see the crocodiles. Which result would be a cost?</td>\n",
                            "      <td>Josiah will give up the chance to see the emus. He would have enjoyed seeing them more than the ...</td>\n",
                            "      <td>Josiah will give up the chance to see the emus. He would have enjoyed seeing them more than the ...</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>Which logical fallacy is used in the text?\\nPresident Hamilton is an effective communicator, bec...</td>\n",
                            "      <td>circular reasoning: an argument that supports a claim with the claim itself</td>\n",
                            "      <td>circular reasoning: an argument that supports a claim with the claim itself</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>Select the invertebrate.</td>\n",
                            "      <td>black orb weaver spider</td>\n",
                            "      <td>black orb weaver spider</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>Which correctly shows the title of a movie?</td>\n",
                            "      <td>***Return of the Jedi***</td>\n",
                            "      <td>***Return of the Jedi***</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>Select the one substance that is not a rock.</td>\n",
                            "      <td>Steel is made by humans. It is not a pure substance.</td>\n",
                            "      <td>Steel is made by humans. It is not a pure substance.</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>5</th>\n",
                            "      <td>Complete the sentence so that it uses personification.\\nThe alarm () the burglar, scaring him away.</td>\n",
                            "      <td>screamed at</td>\n",
                            "      <td>screamed at</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>6</th>\n",
                            "      <td>Assume all other forces on Francesca are balanced. Which statement describes the forces on Franc...</td>\n",
                            "      <td>The forces are unbalanced, so there is a net force on Francesca.</td>\n",
                            "      <td>The forces are unbalanced, so there is a net force on Francesca.</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>7</th>\n",
                            "      <td>Which of the following contains a vague pronoun reference?</td>\n",
                            "      <td>Leah roomed with Olivia last year, but her messiness became a point of contention.</td>\n",
                            "      <td>Leah roomed with Olivia last year, but her messiness became a point of contention.</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>8</th>\n",
                            "      <td>Which type of sentence is this?\\nAn avid reader, Percy attends weekly book club meetings, and he...</td>\n",
                            "      <td>compound</td>\n",
                            "      <td>compound</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>9</th>\n",
                            "      <td>Which logical fallacy is used in the text?\\nHow do I know that Carla is the most intelligent per...</td>\n",
                            "      <td>circular reasoning: an argument that supports a claim with the claim itself</td>\n",
                            "      <td>circular reasoning: an argument that supports a claim with the claim itself</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                                                                              question  \\\n",
                            "0                          Suppose Josiah decides to see the crocodiles. Which result would be a cost?   \n",
                            "1  Which logical fallacy is used in the text?\\nPresident Hamilton is an effective communicator, bec...   \n",
                            "2                                                                             Select the invertebrate.   \n",
                            "3                                                          Which correctly shows the title of a movie?   \n",
                            "4                                                         Select the one substance that is not a rock.   \n",
                            "5  Complete the sentence so that it uses personification.\\nThe alarm () the burglar, scaring him away.   \n",
                            "6  Assume all other forces on Francesca are balanced. Which statement describes the forces on Franc...   \n",
                            "7                                           Which of the following contains a vague pronoun reference?   \n",
                            "8  Which type of sentence is this?\\nAn avid reader, Percy attends weekly book club meetings, and he...   \n",
                            "9  Which logical fallacy is used in the text?\\nHow do I know that Carla is the most intelligent per...   \n",
                            "\n",
                            "                                                                                     ground_truth_text  \\\n",
                            "0  Josiah will give up the chance to see the emus. He would have enjoyed seeing them more than the ...   \n",
                            "1                          circular reasoning: an argument that supports a claim with the claim itself   \n",
                            "2                                                                              black orb weaver spider   \n",
                            "3                                                                             ***Return of the Jedi***   \n",
                            "4                                                 Steel is made by humans. It is not a pure substance.   \n",
                            "5                                                                                          screamed at   \n",
                            "6                                     The forces are unbalanced, so there is a net force on Francesca.   \n",
                            "7                   Leah roomed with Olivia last year, but her messiness became a point of contention.   \n",
                            "8                                                                                             compound   \n",
                            "9                          circular reasoning: an argument that supports a claim with the claim itself   \n",
                            "\n",
                            "                                                                                        predicted_text  \\\n",
                            "0  Josiah will give up the chance to see the emus. He would have enjoyed seeing them more than the ...   \n",
                            "1                          circular reasoning: an argument that supports a claim with the claim itself   \n",
                            "2                                                                              black orb weaver spider   \n",
                            "3                                                                             ***Return of the Jedi***   \n",
                            "4                                                 Steel is made by humans. It is not a pure substance.   \n",
                            "5                                                                                          screamed at   \n",
                            "6                                     The forces are unbalanced, so there is a net force on Francesca.   \n",
                            "7                   Leah roomed with Olivia last year, but her messiness became a point of contention.   \n",
                            "8                                                                                             compound   \n",
                            "9                          circular reasoning: an argument that supports a claim with the claim itself   \n",
                            "\n",
                            "   is_correct  \n",
                            "0        True  \n",
                            "1        True  \n",
                            "2        True  \n",
                            "3        True  \n",
                            "4        True  \n",
                            "5        True  \n",
                            "6        True  \n",
                            "7        True  \n",
                            "8        True  \n",
                            "9        True  "
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Create DataFrame\n",
                "df = pd.DataFrame(results)\n",
                "\n",
                "# Calculate Accuracy\n",
                "accuracy = df[\"is_correct\"].mean()\n",
                "print(f\"\\nOverall Accuracy: {accuracy:.2%}\")\n",
                "\n",
                "# Display Results\n",
                "pd.set_option('display.max_colwidth', 100)\n",
                "display(df[[\"question\", \"ground_truth_text\", \"predicted_text\", \"is_correct\"]])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "No incorrect predictions in this sample!\n"
                    ]
                }
            ],
            "source": [
                "# Inspect Reasoning for Incorrect Answers\n",
                "incorrect = df[~df[\"is_correct\"]]\n",
                "if not incorrect.empty:\n",
                "    print(\"\\n--- Analysis of Incorrect Predictions ---\")\n",
                "    for _, row in incorrect.iterrows():\n",
                "        print(f\"\\nQ: {row['question']}\")\n",
                "        print(f\"True: {row['ground_truth_text']} | Pred: {row['predicted_text']}\")\n",
                "        print(f\"Reasoning: {row['reasoning_quality']}\")\n",
                "else:\n",
                "    print(\"No incorrect predictions in this sample!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "PGM",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
